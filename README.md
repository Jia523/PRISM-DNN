**<h1>PIST-DNN: A Two-Stage Deep Learning Framework for Pixel-wise Aerosol Inversion</h1>** 
This tutorial provides step-by-step guidance on how to use **PIST-DNN**, a two-stage deep learning framework designed for satellite-based aerosol retrieval. The framework includes:
* **Pretraining** using pixel-wise data without ground truth
* **Finetuning** using data with real station observations

All input datasets are allowed to contain missing values > (NaN). The code is structured to handle this seamlessly.

---
**<h2>ğŸ“ Overview of File Structure</h2>**
```
 â”œâ”€â”€ config/                        # YAML configs for each stage
 â”‚   â”œâ”€â”€ pretrain.yaml              # config for pretraining
 â”‚   â”œâ”€â”€ finetune.yaml              # config for finetuning
 â”‚   â””â”€â”€ finetune_inference.yaml    # config for inference
 â”œâ”€â”€ data/
 â”‚   â”œâ”€â”€ pretrain/                  # input data for pretraining
 â”‚   â””â”€â”€ train/                     # input data for finetuning
 â”œâ”€â”€ gtm/                           
 â”‚   â”œâ”€â”€ config.py
 â”‚   â”œâ”€â”€ dataset.py
 â”‚   â”œâ”€â”€ model.py
 â”‚   â””â”€â”€ pipeline.py
 â”œâ”€â”€ outputs/                      
 â”œâ”€â”€ main.py                       
 â”œâ”€â”€ environment.yml                # Python environment config
 â””â”€â”€ README.md
```

---
**<h2>ğŸ› ï¸ Environment Setup</h2>**
```
conda env create -f environment.yml
conda activate pist-dnn
```

---
**<h2>1ï¸âƒ£ Step 1: Pretraining (Unsupervised)</h2>**
The pretraining stage trains the model using only pixel-level features, without requiring station observations. This helps the model learn general spatial-temporal representations.   
**<h3>Data Requirements</h3>**  
* **Format:** ```.csv```, ```.feather```, or ```.pkl```
* **File:** placed under ```data/pretrain/pretrain_data.feather```
* **Features:**
  * ```input_cont_cols```: continuous variables for the current pixel
  * ```input_cate_cols```: categorical variables for the current pixel
  * ```space_target_cols```: features from surrounding 8 pixels (spatial context)
  * ```time_target_cols```: features from adjacent days (temporal context)  

**<h3>Config Setup:</h3>** ```config/pretrain.yaml```
```
pretrain_data_path: "data/pretrain/pretrain_data.feather"
output_folder_path: "outputs/pretrain"

input_cont_cols: [...continuous variables...]
input_cate_cols: [...categorical variables...]
space_target_cols: [... spatial features ...]
time_target_cols: [... temporal features ...]

max_epochs:      # maximum number of training epochs
patience:        # number of epochs to wait before early stopping
```
**<h3>Run Pretraining</h3>**
```python main.py -t pretrain -c config/pretrain.yaml```
**<h3>Output</h3>**
Checkpoint will be saved to:  
```outputs/pretrain/lightning_logs/version_X/checkpoints/epoch=XX-step=XXXXX.ckpt```

---
**<h2>2ï¸âƒ£ Step 2: Finetuning (Supervised)</h2>**
The finetuning stage refines the pretrained model using ground-truth station data. This is a supervised training phase.
**<h3>Data Requirements</h3>**
* **Format:** ```.csv```, placed under ```data/train/```ï¼Œeg.:
  * faod+caod_2001-2024_train_beijing.csv
  * faod+caod_2001-2024_valid_beijing.csv
  * faod+caod_2001-2024_test_beijing.csv
* **Features:**
  * ```input_cont_cols:``` continuous features of the current pixel
  * ```input_cate_cols:``` categorical features (e.g., month)
  * ```task_target_cols:``` target values (e.g., faod, caod)
  * 
**<h3>Config Setup:</h3>** ```config/finetune.yaml```
```
train_data_path: data/train/faod+caod_2001-2024_train_beijing.csv
valid_data_path: data/train/faod+caod_2001-2024_valid_beijing.csv
test_data_path: data/train/faod+caod_2001-2024_test_beijing.csv

scaler_path: outputs/pretrain/input_cont_scaler.pkl
encoder_path: outputs/pretrain/input_cate_encoders.pkl

pretrain_ckpt_path: outputs/pretrain/lightning_logs/version_X/checkpoints/epoch=XX-step=XXXXX.ckpt

input_cont_cols: [...continuous variables...]
input_cate_cols: [...categorical variables...]
task_target_cols: [...target variables...]

max_epochs:      # maximum number of training epochs
patience:        # number of epochs to wait before early stopping
```
**<h3>Run Finetuning</h3>**
```python main.py -t finetune -c config/finetune.yaml```
**<h3>Output</h3>**
* Predictions on test set
* 128 ```x_feature_*``` vectors saved with test results

---
**<h2>3ï¸âƒ£ Step 3: Visualize Model Outputs</h2>**
After testing, the output CSV will contain 128 ```x_feature_*``` columns. You can apply dimension reduction techniques to visualize the learned feature space and assess separation by season or region:  
* t-SNE
* PCA
* UMAP

**<h2>ğŸ“ Citation</h2>**
If you use this framework, please cite or acknowledge the corresponding paper.
